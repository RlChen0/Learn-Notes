# 实验记录

开始时间：2021/10/9 18:51

环境：basenji

工作目录：~/workspace

## 目的

利用basenji_motifs.py 绘制motif，heatmap。

## 命令及结果

### version1

```shell
bsub -q gpu -m gpu01 -o ./report/20211009/motif_1_out -e ./report/20211009/motif_1_err "basenji_motifs.py -d --heat -o basenji2_motif --tfr valid ./params/params_131k_24.json ./models/basenji2_IRGSP_24/model_best.h5 ./data/IRGSP20/"
```

#### error

```python
Traceback (most recent call last):
  File "/public/home/xwli/workspace/basenji/bin/basenji_motifs.py", line 793, in <module>
    main()
  File "/public/home/xwli/workspace/basenji/bin/basenji_motifs.py", line 79, in main
    default='%s/cisbp/Homo_sapiens.meme' % os.environ['HG38'],
  File "/public/home/xwli/anaconda3/envs/basenji/lib/python3.8/os.py", line 675, in __getitem__
    raise KeyError(key) from None
KeyError: 'HG38'
```

#### 原因

未设定motif 注释数据

#### 处理

下载注释数据 

```shell
wget https://meme-suite.org/meme/meme-software/Databases/motifs/motif_databases.12.21.tgz
```

解压

```shell
tar -xzvf motif_databases.12.21.tgz 
```

### version2

```shell
bsub -q gpu -m gpu01 -o ./report/20211009/motif_2_out -e ./report/20211009/motif_2_err "basenji_motifs.py -d --heat -m ./data/motif_databases/CIS-BP_2.00/Oryza_sativa.meme -o basenji2_motif --tfr valid ./params/params_131k_24.json ./models/basenji2_IRGSP_24/model_best.h5 ./data/IRGSP20/"
```

#### error

与version 1 报错相同

##### 处理

修改basenji_motifs.py 

```python
  parser.add_option('-m', dest='meme_db',
      default='%s/cisbp/Homo_sapiens.meme' % os.environ['HG38'],
      help='MEME database used to annotate motifs')
```



```python
  parser.add_option('-m', dest='meme_db',
      default='/cisbp/Homo_sapiens.meme' ,
      help='MEME database used to annotate motifs')
```

#### error2

```python
Traceback (most recent call last):
  File "/public/home/xwli/workspace/basenji/bin/basenji_motifs.py", line 793, in <module>
    main()
  File "/public/home/xwli/workspace/basenji/bin/basenji_motifs.py", line 121, in main
    split_label=options.split_label,
AttributeError: 'Values' object has no attribute 'split_label'
```

##### 处理

添加split_label参数

```python
  parser.add_option('--split', dest='split_label',
      default='test',
      help='Dataset split label for eg TFR pattern [Default: %default]')
```

### version3

```shell
bsub -q gpu -m gpu01 -o ./report/20211009/motif_3_out -e ./report/20211009/motif_3_err "basenji_motifs.py -d --heat --split valid -m ./data/motif_databases/CIS-BP_2.00/Oryza_sativa.meme -o basenji2_motif --tfr valid ./params/params_131k_24.json ./models/basenji2_IRGSP_24/model_best.h5 ./data/basenji2_data/IRGSP20"
```

#### error

```python
Traceback (most recent call last):
  File "/public/home/xwli/workspace/basenji/bin/basenji_motifs.py", line 796, in <module>
    main()
  File "/public/home/xwli/workspace/basenji/bin/basenji_motifs.py", line 123, in main
    eval_data = dataset.SeqDataset(data_dir,
  File "/public/home/xwli/workspace/basenji/basenji/dataset.py", line 61, in __init__
    self.compute_stats()
  File "/public/home/xwli/workspace/basenji/basenji/dataset.py", line 158, in compute_stats
    dataset = tf.data.Dataset.list_files(self.tfr_path)
  File "/public/home/xwli/anaconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py", line 1224, in list_files
    assert_not_empty = control_flow_ops.Assert(
  File "/public/home/xwli/anaconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py", line 201, in wrapper
    return target(*args, **kwargs)
  File "/public/home/xwli/anaconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py", line 247, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs),
  File "/public/home/xwli/anaconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/ops/control_flow_ops.py", line 154, in Assert
    raise errors.InvalidArgumentError(
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: ./data/basenji2_data/IRGSP20/tfrecords/valid'

```

--tfr 参数为添加子目录

### version4

```shell
bsub -q gpu -m gpu01 -o ./report/20211009/motif_4_out -e ./report/20211009/motif_4_err "basenji_motifs.py -d --heat --split valid -m ./data/motif_databases/CIS-BP_2.00/Oryza_sativa.meme -o basenji2_motif ./params/params_131k_24.json   ./models/basenji2_IRGSP_24/model_best.h5 ./data/basenji2_data/IRGSP20"
```

#### error

```python
Traceback (most recent call last):
  File "/public/home/xwli/workspace/basenji/bin/basenji_motifs.py", line 796, in <module>
    main()
  File "/public/home/xwli/workspace/basenji/bin/basenji_motifs.py", line 179, in main
    plot_filter_logo(filter_outs[:, :, f], filter_size,
IndexError: index 24 is out of bounds for axis 2 with size 24

```

#### 处理

发现basenji_motifs.py 中filters_out 为最后输出层

```python

```

#### error2

显存不足

```python
tensorflow.python.framework.errors_impl.ResourceExhaustedError:  OOM when allocating tensor with shape[4,1,131072,288] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node model_2/conv1d/conv1d-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
 [Op:__inference_predict_function_2995]
```

将batch_size 修改为1

仍旧不足

提交至high 使用CPU运行

报错

缺少tomtom软件，安装meme

```shell
       ========================
        Configuration parameters
        ========================

  Install path:               /public/home/xwli/meme
  Install UID:                
  Version:                    5.4.1
  C compiler:                 gcc
  C compiler flags:           -std=gnu89 -fno-common -Wall -Wno-unused -DUNIX -D__USE_FIXED_PROTOTYPES__   -O3
  Linker:                     /bin/ld -m elf_x86_64
  Special Libs:               -lz -lm 
  MPICC:                      gcc
  MPIRUN:                     
  MPI_CMD:                    
  MPIINC:                     
  MPILIBDIR:                  
  MPIFLAGS:                   
  MPI_NPROCS:                 1
  OPAL URL:                   no
  OPAL DEPLOY DIRECTORY:      
  BUILD LIBXML2:              yes
  LIBXML2 compiler flags:     -I${top_srcdir}/src/libxml2/include
  LIBXML2 libs:               ${top_builddir}/src/libxml2/libxml2.la
  BUILD LIBXSLT:              yes
  LIBXSLT compiler flags:     -I${top_srcdir}/src/
  LIBXSLT libs:               ${top_builddir}/src/libxslt/libxslt.la
  SOURCE URL:		      https://meme-suite.org
  WEBSITE URL:                https://meme-suite.org/meme
  ALTERNATE WEBSITE URL:      
  PREVIOUS VERSION URL:       
  PREVIOUS VERSION:           
  NOTICES URL:                ./notices.txt
  NEWS URL:                   ./news.txt
  WEBSITE CONTACT:            
  DEVELOPER CONTACT:          meme-suite@uw.edu
  GO-SERVER URL:              http://amigo.geneontology.org/amigo/term/GO_TERM_ID
  MEME DB:                    ${datarootdir}/${PACKAGE_NAME}-5.4.1/db
  MEME LOGS:                  ${localstatedir}/${PACKAGE_NAME}-5.4.1/LOGS
  MEME TEMP FILES:            
  PERL:                       /bin/perl
  PYTHON:                     /public/home/xwli/anaconda3/envs/basenji/bin/python
  PYTHON VERSiON:             3.8
  CONVERT:                    
  GHOSTSCRIPT:                /bin/gs
  QUOTA:                      
  EXPIRY:                     4
  DRMAA QUEUE:                
  DRMAA QUEUE SHORT:          
  MAXTIME:                    14400
  MAXTIME SHORT:              300
  MAXMEMORY:                  4
  MAXMEMORY SHORT:            1
  STREME LENGTH FACTOR:       1e6

  Run the following commands to compile, test and install meme:
        make
        make test
        make install

  Then make sure that the following two directories are added to
  your PATH variable:
        /public/home/xwli/meme/bin
        /public/home/xwli/meme/libexec/meme-5.4.1

  This can often be done by editing the file named .profile to add
  the following line:
	export PATH=/public/home/xwli/meme/bin:/public/home/xwli/meme/libexec/meme-5.4.1:$PATH

```

### version 5

```shell
bsub -q high -R "rusage[mem=50GB]" -o ./report/20211011/motif_out1 -e ./report/20211011/motif_e1 "basenji_motifs.py -d --heat --split valid -m ./data/motif_databases/CIS-BP_2.00/Oryza_sativa.meme -o basenji2_motif ./params/params_131k_24.json   ./models/basenji2_IRGSP_24/model_best.h5 ./data/basenji2_data/IRGSP20"
```

```shell
bsub -q gpu -m gpu01 -o  "basenji_motifs.py -d --heat --split valid -t 250 -m ./data/motif_databases/CIS-BP_2.00/Oryza_sativa.meme -o basenji2_motif ./params/params_131k_24.json   ./models/basenji2_IRGSP_24/model_best.h5 ./data/basenji2_data/IRGSP20"

```

```shell
bsub -q gpu -m gpu01 -o ./report/20211015/motif_out1 -e ./report/20211015/motif_err1 "basenji_motifs_inject.py ./params/params_131k_24.json   ./models/basenji2_IRGSP_24/model_best.h5 ./data/basenji2_data/IRGSP20"
```

```shell
bsub -q gpu -m gpu01 -o ./report/20211015/motifout1 -e ./report/20211015/motiferr1  "basenji_motifs.py -d --heat --split valid -t -l 1024 -m ./data/motif_databases/CIS-BP_2.00/Oryza_sativa.meme -o basenji2_motif ./params/params_131k_24.json   ./models/basenji2_IRGSP_24/model_best.h5 ./data/basenji2_data/IRGSP20_"
```

# Enformer训练

智心云

A100

1536 // 4, max

cktp1 {'PearsonR': 0.6058981, 'R2': -0.25548255}
cktp2 {'PearsonR': 0.63998675, 'R2': 0.117900796}
cktp3 {'PearsonR': 0.6560181, 'R2': 0.37653232}
cktp4 {'PearsonR': 0.6905794, 'R2': 0.464722}
cktp5 {'PearsonR': 0.71932644, 'R2': 0.5139991}
cktp6 {'PearsonR': 0.6269238, 'R2': 0.39000535}
cktp7 {'PearsonR': 0.7307067, 'R2': 0.52754307}

1000 step/epoch
cktp8 {'PearsonR': 0.734796, 'R2': 0.5402675}
ckpt9 {'PearsonR': 0.74144363, 'R2': 0.54697526}
ckpt10 {'PearsonR': 0.7099864, 'R2': 0.48001623}



1536  , max

```python
-----------Restoring from ./tf_ckpts/ckpt-1-----------
144it [00:20,  7.15it/s]

{'PearsonR': 0.5985855, 'R2': 0.062898226}

-----------Restoring from ./tf_ckpts/ckpt-2-----------
144it [00:19,  7.21it/s]

{'PearsonR': 0.55314046, 'R2': -1.7966113}

-----------Restoring from ./tf_ckpts/ckpt-3-----------
144it [00:20,  7.19it/s]

{'PearsonR': 0.40583602, 'R2': 0.14245874}

-----------Restoring from ./tf_ckpts/ckpt-4-----------
144it [00:20,  7.13it/s]

{'PearsonR': 0.18016331, 'R2': -0.5729881}

-----------Restoring from ./tf_ckpts/ckpt-5-----------
144it [00:20,  7.11it/s]

{'PearsonR': 0.3387508, 'R2': 0.042900413}


```

# Enformer Cross speices



```python
Saved checkpoint for Epoch 1, steps 1000, save path: ./tf_ckpts/ckpt-1
Epoch 1/1
loss_rice 0.317706 loss_maize 0.075626 learning_rate 0.000100000005
-----------Restoring from ./tf_ckpts/ckpt-1-----------
101it [00:25,  3.93it/s]
{'PearsonR': 0.34042385, 'R2': -1.6142834}
-----------Restoring from ./tf_ckpts/ckpt-1-----------
101it [00:24,  4.06it/s]

```

```python
Saved checkpoint for Epoch 2, steps 2000, save path: ./tf_ckpts/ckpt-2
Epoch 1/1
loss_rice 0.531779 loss_maize 0.089536 learning_rate 0.00020000001
-----------Restoring from ./tf_ckpts/ckpt-2-----------
101it [00:24,  4.06it/s]
{'PearsonR': 0.52025986, 'R2': -0.48210382}
-----------Restoring from ./tf_ckpts/ckpt-2-----------
101it [00:24,  4.07it/s]
{'PearsonR': 0.50644433, 'R2': -0.24873589}
```

```python
Saved checkpoint for Epoch 3, steps 3000, save path: ./tf_ckpts/ckpt-3
Epoch 1/1
loss_rice 0.303073 loss_maize 0.089015 learning_rate 0.0003
-----------Restoring from ./tf_ckpts/ckpt-3-----------
101it [00:25,  3.96it/s]
{'PearsonR': 0.4271849, 'R2': 0.08723086}
-----------Restoring from ./tf_ckpts/ckpt-3-----------
101it [00:25,  4.03it/s]
{'PearsonR': 0.55109847, 'R2': 0.039844163}

```

```python
Saved checkpoint for Epoch 4, steps 4000, save path: ./tf_ckpts/ckpt-4
Epoch 1/1
loss_rice 0.275987 loss_maize 0.042004 learning_rate 0.00040000002
-----------Restoring from ./tf_ckpts/ckpt-4-----------
101it [00:25,  3.96it/s]
rice
{'PearsonR': 0.46838585, 'R2': 0.20177285}
-----------Restoring from ./tf_ckpts/ckpt-4-----------
101it [00:24,  4.09it/s]
maize
{'PearsonR': 0.44104767, 'R2': 0.13556808}
```

```python
Saved checkpoint for Epoch 5, steps 5000, save path: ./tf_ckpts/ckpt-5
Epoch 1/1
loss_rice 0.271647 loss_maize 0.043280 learning_rate 0.0005
-----------Restoring from ./tf_ckpts/ckpt-5-----------
101it [00:25,  4.04it/s]
rice
{'PearsonR': 0.22915922, 'R2': -0.77698237}
-----------Restoring from ./tf_ckpts/ckpt-5-----------
101it [00:24,  4.08it/s]
maize
{'PearsonR': 0.29445237, 'R2': -2.0149436}
```

```python
Saved checkpoint for Epoch 6, steps 6000, save path: ./tf_ckpts/ckpt-6
Epoch 1/1
loss_rice 0.292983 loss_maize 0.060422 learning_rate 0.0005
-----------Restoring from ./tf_ckpts/ckpt-6-----------
101it [00:24,  4.04it/s]
rice
{'PearsonR': 0.18142153, 'R2': -0.6660679}
-----------Restoring from ./tf_ckpts/ckpt-6-----------
101it [00:24,  4.11it/s]
maize
{'PearsonR': 0.19811061, 'R2': -4.721236}
```

```python
Saved checkpoint for Epoch 7, steps 7000, save path: ./tf_ckpts/ckpt-7
Epoch 1/1
loss_rice 0.281201 loss_maize 0.043811 learning_rate 0.0005
-----------Restoring from ./tf_ckpts/ckpt-7-----------
101it [00:25,  4.03it/s]
rice
{'PearsonR': 0.19378382, 'R2': -0.81560606}
-----------Restoring from ./tf_ckpts/ckpt-7-----------
101it [00:24,  4.07it/s]
maize
{'PearsonR': 0.15456034, 'R2': -1.6157167}

```

```
Saved checkpoint for Epoch 8, steps 8000, save path: ./tf_ckpts/ckpt-8
Epoch 1/33
loss_rice 0.276363 loss_maize 0.056936 learning_rate 0.0005

101it [00:39,  2.54it/s]
rice
{'PearsonR': 0.18242262, 'R2': -0.433131}
-----------Restoring from ./tf_ckpts/ckpt-8-----------
101it [00:24,  4.11it/s]
maize
{'PearsonR': 0.041593507, 'R2': -2.6279154}
```

```
Saved checkpoint for Epoch 9, steps 9000, save path: ./tf_ckpts/ckpt-9
Epoch 2/33
loss_rice 0.314362 loss_maize 0.180383 learning_rate 0.0005

101it [00:38,  2.62it/s]
rice
{'PearsonR': 0.036765, 'R2': -5.9196115}
-----------Restoring from ./tf_ckpts/ckpt-9-----------
101it [00:24,  4.11it/s]
maize
{'PearsonR': -0.025077213, 'R2': -55.04471}
```

```
Saved checkpoint for Epoch 10, steps 10000, save path: ./tf_ckpts/ckpt-10
Epoch 3/33
loss_rice 0.299943 loss_maize 0.095257 learning_rate 0.0005

101it [00:39,  2.59it/s]
rice
{'PearsonR': 0.021926, 'R2': -17.723969}
-----------Restoring from ./tf_ckpts/ckpt-10-----------
101it [00:24,  4.12it/s]
maize
{'PearsonR': -0.031949736, 'R2': -256.62183}
```

```
Saved checkpoint for Epoch 11, steps 11000, save path: ./tf_ckpts/ckpt-11
Epoch 4/33
loss_rice 0.287830 loss_maize 0.063815 learning_rate 0.0005

101it [00:39,  2.58it/s]
rice
{'PearsonR': -0.041795254, 'R2': -155.04428}
-----------Restoring from ./tf_ckpts/ckpt-11-----------
101it [00:24,  4.07it/s]
maize
{'PearsonR': -0.039727658, 'R2': -739.38696}
```

```
Saved checkpoint for Epoch 12, steps 12000, save path: ./tf_ckpts/ckpt-12
Epoch 5/33
loss_rice 0.299628 loss_maize 0.058677 learning_rate 0.0005

101it [00:39,  2.53it/s]
rice
{'PearsonR': -0.07138685, 'R2': -273.29117}
-----------Restoring from ./tf_ckpts/ckpt-12-----------
101it [00:24,  4.05it/s]
maize
{'PearsonR': -0.012468948, 'R2': -796.44543}
```

```
Saved checkpoint for Epoch 13, steps 13000, save path: ./tf_ckpts/ckpt-13
Epoch 6/33
loss_rice 0.356569 loss_maize 0.188051 learning_rate 0.0005

101it [00:39,  2.55it/s]
rice
{'PearsonR': -0.03453781, 'R2': -145.3445}
-----------Restoring from ./tf_ckpts/ckpt-13-----------
101it [00:24,  4.10it/s]
maize
{'PearsonR': -0.012434984, 'R2': -717.44476}

```

```
Saved checkpoint for Epoch 14, steps 14000, save path: ./tf_ckpts/ckpt-14
Epoch 7/33
loss_rice 0.215569 loss_maize 0.092030 learning_rate 0.0005

101it [00:39,  2.58it/s]
rice
{'PearsonR': -0.01597319, 'R2': -292.32217}
-----------Restoring from ./tf_ckpts/ckpt-14-----------
101it [00:24,  4.08it/s]
maize
{'PearsonR': -0.044019282, 'R2': -3375.1724}

```

```
Saved checkpoint for Epoch 15, steps 15000, save path: ./tf_ckpts/ckpt-15
Epoch 8/33
loss_rice 0.342091 loss_maize 0.105992 learning_rate 0.0005

101it [00:39,  2.54it/s]
rice
{'PearsonR': -0.034262054, 'R2': -151.87251}
-----------Restoring from ./tf_ckpts/ckpt-15-----------
101it [00:24,  4.11it/s]
maize
{'PearsonR': -0.043443114, 'R2': -483.24097}
```

```
Saved checkpoint for Epoch 16, steps 16000, save path: ./tf_ckpts/ckpt-16
Epoch 9/33
loss_rice 0.235701 loss_maize 0.126471 learning_rate 0.0005

rice
{'PearsonR': -0.0114506325, 'R2': -58.293304}
-----------Restoring from ./tf_ckpts/ckpt-16-----------
101it [00:24,  4.09it/s]
maize
{'PearsonR': -0.041968573, 'R2': -500.0287}
```

```
Saved checkpoint for Epoch 17, steps 17000, save path: ./tf_ckpts/ckpt-17
Epoch 10/33
loss_rice 0.267744 loss_maize 0.145582 learning_rate 0.0005

101it [00:39,  2.57it/s]
rice
{'PearsonR': -0.062059563, 'R2': -872.9793}
-----------Restoring from ./tf_ckpts/ckpt-17-----------
101it [00:24,  4.09it/s]
maize
{'PearsonR': -0.04786001, 'R2': -3266.291}
```

```
Saved checkpoint for Epoch 18, steps 18000, save path: ./tf_ckpts/ckpt-18
Epoch 11/33
loss_rice 0.318467 loss_maize 0.088682 learning_rate 0.0005

101it [00:38,  2.61it/s]
rice
{'PearsonR': -0.047810722, 'R2': -1850.3861}
-----------Restoring from ./tf_ckpts/ckpt-18-----------
101it [00:24,  4.13it/s]
maize
{'PearsonR': -0.050025254, 'R2': -6826.849}
```

```
Saved checkpoint for Epoch 19, steps 19000, save path: ./tf_ckpts/ckpt-19
Epoch 12/33
loss_rice 0.336703 loss_maize 0.079147 learning_rate 0.0005

101it [00:40,  2.52it/s]
rice
{'PearsonR': -0.05848476, 'R2': -1227.1349}
-----------Restoring from ./tf_ckpts/ckpt-19-----------
101it [00:25,  4.04it/s]
maize
{'PearsonR': -0.04110259, 'R2': -3840.0808}
```

```
Saved checkpoint for Epoch 20, steps 20000, save path: ./tf_ckpts/ckpt-20
Epoch 13/33
loss_rice 0.368870 loss_maize 0.125792 learning_rate 0.0005

101it [00:38,  2.65it/s]
rice
{'PearsonR': -0.1095298, 'R2': -8196.465}
-----------Restoring from ./tf_ckpts/ckpt-21-----------
101it [00:24,  4.09it/s]
maize
{'PearsonR': -0.089123696, 'R2': -52606.395}
```

```
Saved checkpoint for Epoch 21, steps 21000, save path: ./tf_ckpts/ckpt-21
Epoch 14/33
loss_rice 0.395916 loss_maize 0.053131 learning_rate 0.0005

101it [00:42,  2.38it/s]
rice
{'PearsonR': -0.10597077, 'R2': -14206.852}
-----------Restoring from ./tf_ckpts/ckpt-22-----------
101it [00:24,  4.10it/s]
maize
{'PearsonR': -0.07454535, 'R2': -65996.17}
```

```
Saved checkpoint for Epoch 22, steps 22000, save path: ./tf_ckpts/ckpt-22
Epoch 15/33
loss_rice 0.408222 loss_maize 0.053208 learning_rate 0.0005

101it [00:39,  2.58it/s]
rice
{'PearsonR': -0.060272437, 'R2': -436.1516}
-----------Restoring from ./tf_ckpts/ckpt-23-----------
101it [00:24,  4.10it/s]
maize
{'PearsonR': -0.046130538, 'R2': -1410.2463}
```

```
Saved checkpoint for Epoch 23, steps 23000, save path: ./tf_ckpts/ckpt-23
Epoch 16/33
loss_rice 0.334816 loss_maize 0.139154 learning_rate 0.0005
101it [00:39,  2.53it/s]
rice
{'PearsonR': -0.058847766, 'R2': -1126.7905}
-----------Restoring from ./tf_ckpts/ckpt-24-----------
101it [00:24,  4.09it/s]
maize
{'PearsonR': -0.029893488, 'R2': -4085.9675}
```

```
Saved checkpoint for Epoch 24, steps 24000, save path: ./tf_ckpts/ckpt-24
Epoch 17/33
loss_rice 0.326321 loss_maize 0.277033 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 25, steps 25000, save path: ./tf_ckpts/ckpt-25
Epoch 18/33
loss_rice 0.257610 loss_maize 0.133554 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 26, steps 26000, save path: ./tf_ckpts/ckpt-26
Epoch 19/33
loss_rice 0.371031 loss_maize 0.062699 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 27, steps 27000, save path: ./tf_ckpts/ckpt-27
Epoch 20/33
loss_rice 0.325820 loss_maize 0.245465 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 28, steps 28000, save path: ./tf_ckpts/ckpt-28
Epoch 21/33
loss_rice 0.404558 loss_maize 0.237193 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 29, steps 29000, save path: ./tf_ckpts/ckpt-29
Epoch 22/33
loss_rice 0.337287 loss_maize 0.172697 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 30, steps 30000, save path: ./tf_ckpts/ckpt-30
Epoch 23/33
loss_rice 0.279860 loss_maize 0.144249 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 31, steps 31000, save path: ./tf_ckpts/ckpt-31
Epoch 24/33
loss_rice 0.377086 loss_maize 0.054878 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 32, steps 32000, save path: ./tf_ckpts/ckpt-32
Epoch 25/33
loss_rice 0.308846 loss_maize 0.041423 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 33, steps 33000, save path: ./tf_ckpts/ckpt-33
Epoch 26/33
loss_rice 0.353799 loss_maize 0.047008 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 34, steps 34000, save path: ./tf_ckpts/ckpt-34
Epoch 27/33
loss_rice 0.403711 loss_maize 0.075020 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 35, steps 35000, save path: ./tf_ckpts/ckpt-35
Epoch 28/33
loss_rice 0.254717 loss_maize 0.070153 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 36, steps 36000, save path: ./tf_ckpts/ckpt-36
Epoch 29/33
loss_rice 0.429552 loss_maize 0.216281 learning_rate 0.0005
```

```
Saved checkpoint for Epoch 37, steps 37000, save path: ./tf_ckpts/ckpt-37
Epoch 30/33
loss_rice 0.413383 loss_maize 0.109954 learning_rate 0.0005
```

# RICE 24 ENFOMER

## channels = 1536 // 2

Enformer 

dataset = rice24

channel=1536//2

```python
Saved checkpoint for Epoch 1, steps 1000, save path: ./ckpt/rice24/tf_ckpts/ckpt-1
Epoch 1/1
loss_rice 0.177947 learning_rate 0.000100000005
-----------Restoring from ./ckpt/rice24/tf_ckpts/ckpt-1-----------
101it [00:24,  4.14it/s]
rice
{'PearsonR': 0.65073663, 'R2': 0.3929503}
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Saved checkpoint for Epoch 2, steps 2000, save path: ./ckpt/rice24/tf_ckpts/ckpt-2
Epoch 1/1
loss_rice 0.349137 learning_rate 0.00020000001
-----------Restoring from ./ckpt/rice24/tf_ckpts/ckpt-2-----------
101it [00:24,  4.08it/s]
rice
{'PearsonR': 0.6120668, 'R2': -0.3852516}
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Saved checkpoint for Epoch 3, steps 3000, save path: ./ckpt/rice24/tf_ckpts/ckpt-3
Epoch 1/1
loss_rice 0.158492 learning_rate 0.0003
-----------Restoring from ./ckpt/rice24/tf_ckpts/ckpt-3-----------
101it [00:25,  4.02it/s]
rice
{'PearsonR': 0.51567763, 'R2': -1.1098047}

```

## channels = 1536 // 4

```
Saved checkpoint for Epoch 1, steps 200, save path: ./ckpt/rice24_384/tf_ckpts/ckpt-1
Epoch 1/1
loss_rice 0.286879 learning_rate 5.0000002e-05
-----------Restoring from ./ckpt/rice24_384/tf_ckpts/ckpt-1-----------
101it [00:16,  6.21it/s]
rice
{'PearsonR': 0.33474538, 'R2': -0.09824845}


Saved checkpoint for Epoch 2, steps 400, save path: ./ckpt/rice24_384/tf_ckpts/ckpt-2
Epoch 1/1
loss_rice 0.249861 learning_rate 0.000100000005
-----------Restoring from ./ckpt/rice24_384/tf_ckpts/ckpt-2-----------
101it [00:16,  5.96it/s]
rice
{'PearsonR': 0.3912065, 'R2': -0.24097979}

Saved checkpoint for Epoch 3, steps 600, save path: ./ckpt/rice24_384/tf_ckpts/ckpt-3
Epoch 1/1
loss_rice 0.280651 learning_rate 0.00015
-----------Restoring from ./ckpt/rice24_384/tf_ckpts/ckpt-3-----------
101it [00:16,  6.03it/s]
rice
{'PearsonR': 0.43174878, 'R2': -0.97559714}


Saved checkpoint for Epoch 4, steps 800, save path: ./ckpt/rice24_384/tf_ckpts/ckpt-4
Epoch 1/1
loss_rice 0.427975 learning_rate 0.00020000001
-----------Restoring from ./ckpt/rice24_384/tf_ckpts/ckpt-4-----------
101it [00:16,  6.05it/s]
rice
{'PearsonR': 0.50888294, 'R2': -1.3149862}


Saved checkpoint for Epoch 5, steps 1000, save path: ./ckpt/rice24_384/tf_ckpts/ckpt-5
Epoch 1/1
loss_rice 0.173410 learning_rate 0.00025
-----------Restoring from ./ckpt/rice24_384/tf_ckpts/ckpt-5-----------
101it [00:16,  6.09it/s]
rice
{'PearsonR': 0.61946255, 'R2': 0.22098689}


Saved checkpoint for Epoch 6, steps 1200, save path: ./ckpt/rice24_384/tf_ckpts/ckpt-6
Epoch 1/1
loss_rice 0.205717 learning_rate 0.0003
-----------Restoring from ./ckpt/rice24_384/tf_ckpts/ckpt-6-----------
101it [00:16,  6.04it/s]
rice
{'PearsonR': 0.5750519, 'R2': -1.1056916}

Saved checkpoint for Epoch 7, steps 1400, save path: ./ckpt/rice24_384/tf_ckpts/ckpt-7
Epoch 1/1
loss_rice 0.494797 learning_rate 0.00035000002
-----------Restoring from ./ckpt/rice24_384/tf_ckpts/ckpt-7-----------
101it [00:16,  6.05it/s]
rice
{'PearsonR': 0.5244512, 'R2': 0.14782558}


Saved checkpoint for Epoch 8, steps 1600, save path: ./ckpt/rice24_384/tf_ckpts/ckpt-8
Epoch 1/1
loss_rice 0.304436 learning_rate 0.00040000002
-----------Restoring from ./ckpt/rice24_384/tf_ckpts/ckpt-8-----------
101it [00:16,  6.03it/s]
rice
{'PearsonR': 0.6499861, 'R2': 0.21666764}

```

